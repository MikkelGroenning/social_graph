{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data used to perform the analysis\n",
    "The precidency of Donald J. Trump began at noon EST (17:00 UTC) on January 20, 2017 when he was inaugurated as the 45th president of the United States, and will com to an end the on January 20, 2021 as he ultimately lost the 2020 presidential election to Joe Biden. It is not far fetched to say it has been bizare precidency compared to the most recent precidencies. \n",
    "\n",
    "It feels like America has been splitted in two the suporters of Trump and those agianst him - Repulicans agianst Democrats. In this project we wanted to explore if our hypothesis of polarization can be seen or rejected by analyzing the Congress of United States behavoir on the social media Twitter including the infamous Twitter account manged by Donald J. Trump. The idea was to analyze the congress tweets from the time of the 45th precidency to explore potential polarization.\n",
    "\n",
    "With access to the Twitter API it is only possible to exctract the most reason 3200 tweets from a given account (3200 tweet do not go far back for many american polticians). However, Twitter's Terms of Service do allow for datasets of tweets ID's to be distributed to third parties (not the full JSON). Luckily we found two sources that keep tweet id open very related to our project and one sources that stored the full length tweets of Donald Trump namely:\n",
    "\n",
    "* ** 115th U.S. Congress Tweet Ids: **\n",
    "    A open dataset with 2,041,399 tweets from the Twitter accounts of members of the 115th U.S. Congress collected in the period of January 27, 2017 and January 2, 2019.  *Littman, Justin, 2017, \"115th U.S. Congress Tweet Ids\", https://doi.org/10.7910/DVN/UIVHQR, Harvard Dataverse, V5.*\n",
    "\n",
    "* ** 116th U.S. Congress Tweet Ids **\n",
    "    A open dataset with 2,817,747 tweets from the Twitter accounts of members of the 116th U.S. Congress collected in the period of January 27, 2019 and May 7, 2020  * Wrubel, Laura; Kerchner, Daniel, 2020, \"116th U.S. Congress Tweet Ids\", https://doi.org/10.7910/DVN/MBOJNS, Harvard Dataverse*\n",
    "\n",
    "* ** Trump Twitter Archive **\n",
    "    A site dedicated to scrape every single tweet from Donald J. Trump. Here we downloaded all tweets in the periods of January 27, 2017 and January 2, 2019 and January 27, 2019 and May 7, 2020. https://www.thetrumparchive.com/\n",
    "\n",
    "\n",
    "Examing the Harvard Dataverse we discovered it contains mostly tweets from the American congress but unfortunally also a number of random profiles. Furthermore, many of tweets are associated with not just congress members but also profiles linked to for instance the different Caucuses, officals account on behalf of all Repulican Senate members etc. Furthermore it was not listed what party the account where associated with. However, we found a list of congress memebers with their party association, whether they act as Senator or Represenative and, most important for the project, their twitter profile (if they have an account the vast majority has.). Two different sources for the 115 and 116 congress have been utilized namely\n",
    "\n",
    "* 116. Congress twitter info: (website) https://triagecancer.org/congressional-social-media \n",
    "* 115. Congress  twitter info : (PDF) https://www.sciencecoalition.org/wp-content/uploads/2018/09/115th-Congress-Twitter-Handles.pdf  \n",
    "\n",
    "\n",
    "\n",
    "This notebook consist of three parts and one smaller part devoted to extrating Trump tweets  id (part 0). So in total:\n",
    "* Part 0: Exstract Trump Tweet IDs\n",
    "    Here the tweet id are exstacted from the tweets made publicly by https://www.thetrumparchive.com/\n",
    "* Part 1: Hydrate Tweets\n",
    "    In this part all the tweet ids from Harvard data archive as well as Trumps tweet id, are hydrated. I.e. the ids are turned back into tweets with metadata. \n",
    "* Part 2: Cleanup\n",
    "    As the Harvard data archive hav \n",
    "* Part 3: Preprocess the twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Exstract Trump Tweet IDs\n",
    "As the site Trump Twitter Achive (https://www.thetrumparchive.com/) store Donald Trump's tweets in a different format than how it typically exstracted from API to combat this we exstracted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.trump_tweet_ids import get_trump_tweet_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trump_tweets1 = pd.read_csv('../Data/raw/tweets/trump_tweets_1st.csv')  \n",
    "df_trump_tweets2 = pd.read_csv('../Data/raw/tweets/trump_tweets_1st.csv')\n",
    "df_trump = pd.concat([df_trump_tweets1, df_trump_tweets2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "11326 tweet ids saved\n"
    }
   ],
   "source": [
    "filepath = \"../Data/raw/tweets/trump_id.txt\"\n",
    "get_trump_tweet_ids(df_trump, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Hydrate Tweets\n",
    "The process of turning tweet ID's into actual tweets with metadata is called *hydration* and requires Twitter delopper account https://developer.twitter.com/en/apply-for-access (which is typically pretty easy to acquire)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from src.tools.twitter_api_credentials import api_key, api_secret_key, access_token, access_token_secret\n",
    "from src.data.hydrate import hydrate_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "try:\n",
    "    redirect_url = auth.get_authorization_url()\n",
    "except tweepy.TweepError:\n",
    "    print('Error! Failed to get request token.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "representatives115 = np.loadtxt(\n",
    "    \"../Data/Raw/Tweets/representatives115.txt\", dtype=int\n",
    ")\n",
    "representatives116 = np.loadtxt(\n",
    "    \"../Data/Raw/Tweets/representatives116.txt\", dtype=int\n",
    ")\n",
    "senators115 = np.loadtxt(\n",
    "    \"../Data/Raw/Tweets/senators115.txt\", dtype=int\n",
    ")\n",
    "senators116 = np.loadtxt(\n",
    "    \"../Data/Raw/Tweets/senators116.txt\", dtype=int\n",
    ")\n",
    "trump = np.loadtxt(\n",
    "    \"../Data/Raw/Tweets/trump_id.txt\", dtype=int\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note running the cell below take $24 \\pm 6$ hours as the twitter API set limits to how much can be exstracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = np.concatenate([representatives115, representatives116, senators115, senators116, trump])\n",
    "filepath = \"../Data/interim/congress.pkl\"\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "hydrate_tweets(\n",
    "    tweet_ids=congress,\n",
    "    filepath=filepath,\n",
    "    api = api\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = pd.read_pickle('../Data/Interim/congress.pkl')\n",
    "twitter_handles = pd.read_table('../Data/Processed/Twitter_Handles_updated.csv', sep = ',')\n",
    "\n",
    "s1 = set(twitter_handles['twitter_display_name'])\n",
    "s2 = set(congress.user_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_overlapping_twitter_profiles = s1 ^ s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure tweets only comes from people that twitter handles exist for. \n",
    "congress = congress[congress.user_name.isin(s1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the periods from Harward:\n",
    "mask = (\n",
    "    #January 27, 2017 and January 2, 2019 \n",
    "    (congress.created_at > '2017-1-27 00:00:00') & (congress.created_at < '2019-1-2 00:00:00')\n",
    "    | \n",
    "    #January 27, 2019 and May 7, 2020 \n",
    "    (congress.created_at > '2019-1-27 00:00:00') & (congress.created_at < '2020-5-7 00:00:00')\n",
    ")\n",
    "congress = congress[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = congress.drop_duplicates(keep='first')\n",
    "congress = congress.sort_values(by='created_at')\n",
    "congress = congress.reset_index(drop=True)\n",
    "congress.to_pickle(\"Data/interim/congress_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tweets ids and convert them to integers\n",
    "ids = list(congress.id.astype(int).values)\n",
    "\n",
    "filepath = \"../Data/raw/tweets/Cleaned_tweet_id.txt\"\n",
    "with open(filepath, 'w') as output:\n",
    "    for row in ids:\n",
    "        output.write(str(row) + '\\n')\n",
    "\n",
    "    print(f'{len(ids)} tweet ids saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1 Shortcut to exstract the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe with tweets of from congress after cleanup contain 60 % rows. \n",
    "\n",
    "Note running the cell below take $10 \\pm 2.5$ hours as the twitter API set limits to how much can be exstracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_tweet_id = np.loadtxt(\"../Data/Raw/Tweets/Cleaned_tweet_id.txt\", dtype=int)\n",
    "filepath = \"../Data/interim/congress_cleaned.pkl\"\n",
    "\n",
    "hydrate_tweets(\n",
    "    tweet_ids=congress_tweet_id,\n",
    "    filepath=filepath,\n",
    "    api = api\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Preprocess the twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = pd.read_pickle('../Data/Interim/congress_cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_characters = \",._´&’%':€$£!?#\"\n",
    "character_set = {\n",
    "    \"characters\": \"abcdefghijklmnopqrstuvwxyz0123456789\" + special_characters,\n",
    "    \"space\": \" \",\n",
    "}\n",
    "alphabet = \"\".join(character_set.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_links = re.compile(\"http\\S+\")\n",
    "regex_whitespace = re.compile(\"[\\s|-]+\")\n",
    "regex_unknown = re.compile(f\"[^{alphabet}]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_html_tags = {\n",
    "    \"&amp\": \"and\",\n",
    "    \"&lt\": \"<\",\n",
    "    \"&gt\": \">\",\n",
    "    \"&quot\": '\"',\n",
    "    \"&apos\": \"'\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace unicode charetars\n",
    "for pattern_string, char in regex_html_tags.items():\n",
    "    congress_tweets[\"text\"] = congress_tweets[\"text\"].str.replace(pattern_string, char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_tweets[\"text\"] = (congress_tweets[\"text\"]\n",
    "    .str.lower()\n",
    "    .str.replace(regex_links, \"\")\n",
    "    .str.replace(regex_whitespace, character_set[\"space\"])\n",
    "    .str.replace(regex_unknown, '')\n",
    "    .str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_tweets.to_pickle('../Data/Processed/congress_cleaned_processed.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitsocialgraphsprojectconda9e1414738f16463880d93a451ffa336f",
   "display_name": "Python 3.7.9 64-bit ('social_graphs_project': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}