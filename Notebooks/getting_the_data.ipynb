{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data used to perform the analysis\n",
    "The precidency of Donald J. Trump began at noon EST (17:00 UTC) on January 20, 2017 when he was inaugurated as the 45th president of the United States, and will com to an end the on January 20, 2021 as he ultimately lost the 2020 presidential election to Joe Biden. It is not far fetched to say it has been bizare precidency compared to the most recent precidencies. \n",
    "\n",
    "It feels like America has been splitted in two the suporters of Trump and those agianst him - Repulicans agianst Democrats. In this project we wanted to explore if our hypothesis of polarization can be seen or rejected by analyzing the Congress of United States behavoir on the social media Twitter including the infamous Twitter account manged by Donald J. Trump. The idea was to analyze the congress tweets from the time of the 45th precidency to explore potential polarization.\n",
    "\n",
    "With access to the Twitter API it is only possible to exctract the most reason 3200 tweets from a given account (3200 tweet do not go far back for many american polticians). However, Twitter's Terms of Service do allow for datasets of tweets ID's to be distributed to third parties (not the full JSON). Luckily we found two sources that keep tweet id open very related to our project and one sources that stored the full length tweets of Donald Trump namely:\n",
    "\n",
    "* **115th U.S. Congress Tweet Ids:**\n",
    "    A open dataset with 2,041,399 tweets from the Twitter accounts of members of the 115th U.S. Congress collected in the period of January 27, 2017 and January 2, 2019. The dataset consists two files of interest namely\n",
    "    * `senators-1.txt` that contains tweet ids for Seneators\n",
    "    * `representatives-1.txt` that contains tweet ids for Representatives\n",
    "    *Littman, Justin, 2017, \"115th U.S. Congress Tweet Ids\", https://doi.org/10.7910/DVN/UIVHQR, Harvard Dataverse, V5.*\n",
    "\n",
    "* **116th U.S. Congress Tweet Ids**\n",
    "    A open dataset with 2,817,747 tweets from the Twitter accounts of members of the 116th U.S. Congress collected in the period of January 27, 2019 and May 7, 2020.  The dataset consists two files of interest namely\n",
    "    * `Senators: congress116-senate-ids.txt` that contains tweet ids for Seneators\n",
    "    * `Representatives: congress116-house-ids.txt` that contains tweet ids for Representatives  * Wrubel, Laura; Kerchner, Daniel, 2020, \"116th U.S. Congress Tweet Ids\", https://doi.org/10.7910/DVN/MBOJNS, Harvard Dataverse*\n",
    "\n",
    "* **Trump Twitter Archive**\n",
    "    A site dedicated to scrape every single tweet from Donald J. Trump. Here we downloaded all tweets in the periods of January 27, 2017 and January 2, 2019 and January 27, 2019 and May 7, 2020. https://www.thetrumparchive.com/\n",
    "\n",
    "Examing the Harvard Dataverse we discovered that polticians in congress can have number of profiles for instance a private profile, a profile associated with work in congress and campaign profile. Unfortunally also a number of random profiles like support profile for certain candidate. Moreover there is tweet dateing as far back as 2008. Furthermore it was not listed what party the account where associated with. However, we found a list of congress memebers with their party association, whether they act as Senator or Represenative and, most important for the project, their twitter profile (if they have an account the vast majority has). That meant rather than dealing with multiple accounts for the same congress member or including random account we would only consider one account per congress member. Two different sources for the 115 and 116 congress have been utilized for this namely\n",
    "\n",
    "* 116. Congress twitter info: (website) https://triagecancer.org/congressional-social-media \n",
    "* 115. Congress twitter info : (PDF) https://www.sciencecoalition.org/wp-content/uploads/2018/09/115th-Congress-Twitter-Handles.pdf  \n",
    "\n",
    "The politicans listed in these to documents as well as Donald Trump are the twitter profiles that ones that will considered. The information needs to scraped as the format is HTML and PDF respectively.\n",
    "\n",
    "## Handeling the Data\n",
    "\n",
    "This notebook will explain how the above described data was exstracted and preprocessed. The notebook consist of four parts and one smaller part devoted to extrating Trump tweets id (part 0). So in total:\n",
    "* **Part 0: Exstract Trump Tweet IDs**\n",
    "    Here the tweet id are exstacted from the tweets made publicly by https://www.thetrumparchive.com/\n",
    "\n",
    "* **Part 1: Hydrate Tweets**\n",
    "    In this part all the tweet ids from Harvard data archive as well as Trumps tweet id, are hydrated. I.e. the ids are turned back into tweets with metadata. \n",
    "\n",
    "* **Part 2: Getting Congress Twitter Account**\n",
    "    Here a pandas data frame is constructed from information scraped from the PDF describing the 115 Congress memebers twitter accounts, and the HTML site describing the 116 Congress Twitter info.\n",
    "\n",
    "* **Part 3: Clean-up of Harward Data**\n",
    "    The Harvard data archive needs to be cleaned prior to analsis as it contains\n",
    "    * Data prior to January 27, 2017\n",
    "    * Duplicates\n",
    "    * Remove random/duplicate twitter accounts\n",
    "    \n",
    "* **Part 4: Preprocess the twitter data**\n",
    "    Many tweets contains links, emojies etc that makes it difficult to perform natural language processing on. In this part the tweets are preprocessed such that they can be used for our analysis.\n",
    "    \n",
    "    \n",
    "Below can all the library dependicies be read. Note the import of functions from our own module `src`. The source code for these functions can be read on our github repository https://github.com/MikkelGroenning/02805_social_graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import camelot\n",
    "import tweepy\n",
    "import tqdm\n",
    "from src.data.trump_tweet_ids import get_trump_tweet_ids\n",
    "from src.data.hydrate import hydrate_tweets\n",
    "from src.tools.twitter_api_credentials import api_key, api_secret_key, access_token, access_token_secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file twitter_api_credentials.py can't be found in out Github repository as it contains classified information about our creditiels to the Twitter API. To recreate the dataset one needs to access the Twitter API through developer account is needed. Such an account can be requested at https://developer.twitter.com/en/apply-for-access typically one is granted access instant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "try:\n",
    "    redirect_url = auth.get_authorization_url()\n",
    "except tweepy.TweepError:\n",
    "    print('Error! Failed to get request token.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Exstract Trump Tweet IDs\n",
    "As the site Trump Twitter Achive (https://www.thetrumparchive.com/) store Donald Trump's tweets in a different format than how it typically exstracted from Twitter-API we exstracted the tweet id from this sources and stored them in the file `trump_id.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trump_tweets1 = pd.read_csv('../Data/raw/tweets/trump_tweets_1st.csv')  \n",
    "df_trump_tweets2 = pd.read_csv('../Data/raw/tweets/trump_tweets_1st.csv')\n",
    "df_trump = pd.concat([df_trump_tweets1, df_trump_tweets2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11326 tweet ids saved\n"
     ]
    }
   ],
   "source": [
    "filepath = \"../Data/raw/tweets/trump_id.txt\"\n",
    "get_trump_tweet_ids(df_trump, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Hydrate Tweets\n",
    "The process of turning tweet ID's into actual tweets with metadata is called *hydration* and requires Twitter delopper account. In the below cell we load all twitter ids obtained from Harward Data Archive and Trump Twitter Archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4870472\n"
     ]
    }
   ],
   "source": [
    "representatives115 = np.loadtxt(\n",
    "    \"../Data/Raw/Tweets/representatives115.txt\", dtype=int\n",
    ")\n",
    "representatives116 = np.loadtxt(\n",
    "    \"../Data/Raw/Tweets/representatives116.txt\", dtype=int\n",
    ")\n",
    "senators115 = np.loadtxt(\n",
    "    \"../Data/Raw/Tweets/senators115.txt\", dtype=int\n",
    ")\n",
    "senators116 = np.loadtxt(\n",
    "    \"../Data/Raw/Tweets/senators116.txt\", dtype=int\n",
    ")\n",
    "trump = np.loadtxt(\n",
    "    \"../Data/Raw/Tweets/trump_id.txt\", dtype=int\n",
    ")\n",
    "congress = np.concatenate([representatives115, representatives116, senators115, senators116, trump])\n",
    "print(len(congress))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concatted into array of tweet id consist of 4.8 millions ID. All these tweet are now hydrated with the function `hydrate_tweets` located in src/data folder in our reposortiry. Note running the cell below take $24 \\pm 6$ hours as the twitter API set limits to how much can be exstracted. More info about rate limits can be found at https://developer.twitter.com/en/docs/twitter-api/v1/rate-limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../Data/interim/congress.pkl\"\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "hydrate_tweets(\n",
    "    tweet_ids=congress,\n",
    "    filepath=filepath,\n",
    "    api = api\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Getting Congress Twitter Account\n",
    "In this part a pandas data frame will generated with each members congress member's name State, Type (Reprensative, sSnator, POTUS), Name, Party. This part cosist of three subparts:\n",
    "* **Part 2.1: 116<sup>th</sup>** Here the desired data frame for 116 congress will be scraped\n",
    "* **Part 2.2: 115<sup>th</sup>** Here the desired data frame for 115 congress will be scraped\n",
    "* **Part 2.3: Merge data** Here the different congress data frame will be merged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1: 116<sup>th</sup> congress\n",
    "\n",
    "First the twitter handles for the 116<sup>th</sup> congress will be extracted using [this](https://triagecancer.org/congressional-social-media) source. The choice of source comes from the fact that the Twitter handle as well as the party is desired.\n",
    "\n",
    "`BeautifulSoup` is used to extract the HTML table from the webpage (that has been downloaded to allow for offline work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data\n",
    "with open('../Data/Raw/116_congress_twitter.html') as fp:\n",
    "    soup = BeautifulSoup(fp, 'html.parser')\n",
    "\n",
    "# Find table\n",
    "table = soup.find('table', attrs={'id':\"footable_16836\"})\n",
    "\n",
    "# Extract data row wise from table\n",
    "l = []\n",
    "for tr in table.findAll('tr'):\n",
    "    td = tr.find_all('td')\n",
    "    row = [tr.text for tr in td]\n",
    "    l.append(row)\n",
    "\n",
    "# Make the data into a Pandas data frame and drop irrelevant columns\n",
    "Data116 = pd.DataFrame(l[1:], columns = [header.getText() for header in table.findAll('th')]).drop(columns = ['Name Links', 'Twitter Links', 'Instagram', 'Facebook Page', 'Facebook'])\n",
    "\n",
    "# Ensure that the type of politician is alligned\n",
    "rename_chamber = {'U.S. Representative': 'Representative', 'U.S. Senator': 'Senator'}\n",
    "Data116 = Data116.replace(rename_chamber).rename(columns = {'Chamber of Congress': 'Type'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data set the state is given as well as congressional district. This is fixed using regex strings as shown below. Moreover the \"@\" are removed from the Twitter handles as the Twitter API does not need it. The vancant positions in Congress are also disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Type</th>\n",
       "      <th>Name</th>\n",
       "      <th>Party</th>\n",
       "      <th>Twitter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>Senator</td>\n",
       "      <td>Richard Shelby</td>\n",
       "      <td>R</td>\n",
       "      <td>SenShelby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>Senator</td>\n",
       "      <td>Doug Jones</td>\n",
       "      <td>D</td>\n",
       "      <td>DougJones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>Representative</td>\n",
       "      <td>Byrne, Bradley</td>\n",
       "      <td>R</td>\n",
       "      <td>RepByrne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL</td>\n",
       "      <td>Representative</td>\n",
       "      <td>Roby, Martha</td>\n",
       "      <td>R</td>\n",
       "      <td>RepMarthaRoby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>Representative</td>\n",
       "      <td>Rogers, Mike</td>\n",
       "      <td>R</td>\n",
       "      <td>RepMikeRogersAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>WI</td>\n",
       "      <td>Representative</td>\n",
       "      <td>Tiffany, Thomas</td>\n",
       "      <td>R</td>\n",
       "      <td>TomTiffanyWI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>WI</td>\n",
       "      <td>Representative</td>\n",
       "      <td>Gallagher, Mike</td>\n",
       "      <td>R</td>\n",
       "      <td>MikeforWI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>WY</td>\n",
       "      <td>Senator</td>\n",
       "      <td>Enzi, Mike</td>\n",
       "      <td>R</td>\n",
       "      <td>SenatorEnzi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>WY</td>\n",
       "      <td>Senator</td>\n",
       "      <td>Barrasso, John</td>\n",
       "      <td>R</td>\n",
       "      <td>SenJohnBarrasso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>WY</td>\n",
       "      <td>Representative</td>\n",
       "      <td>Cheney, Liz</td>\n",
       "      <td>R</td>\n",
       "      <td>Liz_Cheney</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>537 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    State            Type             Name Party          Twitter\n",
       "0      AL         Senator   Richard Shelby     R        SenShelby\n",
       "1      AL         Senator       Doug Jones     D        DougJones\n",
       "2      AL  Representative   Byrne, Bradley     R         RepByrne\n",
       "3      AL  Representative     Roby, Martha     R    RepMarthaRoby\n",
       "4      AL  Representative     Rogers, Mike     R  RepMikeRogersAL\n",
       "..    ...             ...              ...   ...              ...\n",
       "536    WI  Representative  Tiffany, Thomas     R     TomTiffanyWI\n",
       "537    WI  Representative  Gallagher, Mike     R        MikeforWI\n",
       "538    WY         Senator       Enzi, Mike     R      SenatorEnzi\n",
       "539    WY         Senator   Barrasso, John     R  SenJohnBarrasso\n",
       "540    WY  Representative      Cheney, Liz     R       Liz_Cheney\n",
       "\n",
       "[537 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All states abbreviations\n",
    "us_state_abbrev = {\n",
    "    r'Alabama.*': 'AL',\n",
    "    r'Alaska.*': 'AK',\n",
    "    r'American Samoa.*': 'AS',\n",
    "    r'Arizona.*': 'AZ',\n",
    "    r'Arkansas.*': 'AR',\n",
    "    r'California.*': 'CA',\n",
    "    r'Colorado.*': 'CO',\n",
    "    r'Connecticut.*': 'CT',\n",
    "    r'Delaware.*': 'DE',\n",
    "    r'District of Columbia.*': 'DC',\n",
    "    r'Florida.*': 'FL',\n",
    "    r'Georgia.*': 'GA',\n",
    "    r'Guam.*': 'GU',\n",
    "    r'Hawaii.*': 'HI',\n",
    "    r'Idaho.*': 'ID',\n",
    "    r'Illinois.*': 'IL',\n",
    "    r'Indiana.*': 'IN',\n",
    "    r'Iowa.*': 'IA',\n",
    "    r'Kansas.*': 'KS',\n",
    "    r'Kentucky.*': 'KY',\n",
    "    r'Louisiana.*': 'LA',\n",
    "    r'Maine.*': 'ME',\n",
    "    r'Maryland.*': 'MD',\n",
    "    r'Massachusetts.*': 'MA',\n",
    "    r'Michigan.*': 'MI',\n",
    "    r'Minnesota.*': 'MN',\n",
    "    r'Mississippi.*': 'MS',\n",
    "    r'Missouri.*': 'MO',\n",
    "    r'Montana.*': 'MT',\n",
    "    r'Nebraska.*': 'NE',\n",
    "    r'Nevada.*': 'NV',\n",
    "    r'New Hampshire.*': 'NH',\n",
    "    r'New Jersey.*': 'NJ',\n",
    "    r'New Mexico.*': 'NM',\n",
    "    r'New York.*': 'NY',\n",
    "    r'North Carolina.*': 'NC',\n",
    "    r'North Dakota.*': 'ND',\n",
    "    r'Northern Mariana Islands.*':'MP',\n",
    "    r'Ohio.*': 'OH',\n",
    "    r'Oklahoma.*': 'OK',\n",
    "    r'Oregon.*': 'OR',\n",
    "    r'Pennsylvania.*': 'PA',\n",
    "    r'Puerto Rico.*': 'PR',\n",
    "    r'Rhode Island.*': 'RI',\n",
    "    r'South Carolina.*': 'SC',\n",
    "    r'South Dakota.*': 'SD',\n",
    "    r'Tennessee.*': 'TN',\n",
    "    r'Texas.*': 'TX',\n",
    "    r'Utah.*': 'UT',\n",
    "    r'Vermont.*': 'VT',\n",
    "    r'Virgin Islands.*': 'VI',\n",
    "    r'Virginia.*': 'VA',\n",
    "    r'Washington.*': 'WA',\n",
    "    r'West V.*': 'WV', # Written in different ways\n",
    "    r'Wisconsin.*': 'WI',\n",
    "    r'Wyoming.*': 'WY'\n",
    "}\n",
    "\n",
    "# Convert states to two letter abbreviations\n",
    "Data116['State'] = Data116['State'].replace(regex = us_state_abbrev)\n",
    "\n",
    "# Remove @\n",
    "Data116 = Data116.replace(regex = {r'^@': ''})\n",
    "\n",
    "# Remove vacant positions\n",
    "Data116 = Data116[Data116.Name != \"Vacant\"]\n",
    "\n",
    "# Look at the data\n",
    "Data116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also seen that there are an inconsistency in the ways the names are written. This is changed so all names are written with the first name first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data116['Name'] = [name[1][1:]+ \" \" +name[0] if len(name) == 2 else name[0] for name in [name.replace(u'\\xa0', u'').split(',') for name in Data116.Name]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: 115<sup>th</sup> congress\n",
    "\n",
    "Now we move onto the 115th congress. This is data stored in a pdf.table, so for this the `camelot` library is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "file115 = '../Data/Raw/115_congress_twitter.pdf'\n",
    "\n",
    "# Read table across all pages\n",
    "tables = camelot.read_pdf(file115, pages = 'all')\n",
    "\n",
    "# Convert data to pandas data frame\n",
    "Data115 = pd.DataFrame(np.concatenate([d.df.drop(0).values for d in tables]), columns=tables[0].df.iloc[0]).drop(columns = \"District\")\n",
    "\n",
    "# Align chamber name with the 116 data\n",
    "rename_chamber = {'Rep.': 'Representative', 'Sen.': 'Senator'}\n",
    "Data115 = Data115.replace(rename_chamber)\n",
    "\n",
    "# Align name with the 116 data and store it in one column\n",
    "Data115[\"Name\"] = Data115[\"First Name\"] + \" \" + Data115[\"Last Name\"]\n",
    "Data115 = Data115.drop(columns = [\"First Name\", \"Last Name\"])\n",
    "\n",
    "# Align columns name with the 116 data\n",
    "Data115 = Data115.rename(columns = {'Title': 'Type', \"Twitter Handle\": \"Twitter\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3: Merge data\n",
    "\n",
    "Now the two datasets are merged. Here we need to take duplicate acounts into account which accounts for reelections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1072, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge data set\n",
    "Data_Full = Data115.append(Data116, ignore_index = True)\n",
    "\n",
    "# Get shape\n",
    "Data_Full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1072it [16:04,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "to_remove = []\n",
    "twitter_display_name = []\n",
    "for index, handle in tqdm.tqdm(enumerate(Data_Full.Twitter)):\n",
    "    try:\n",
    "        u=api.get_user(handle)\n",
    "    except Exception:\n",
    "        to_remove.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Full = Data_Full.drop(index=to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra duplicate from AS\n",
    "Data_Full = Data_Full[Data_Full.Twitter != 'RepTomPrice']\n",
    "\n",
    "# Drop closed users\n",
    "Data_Full = Data_Full[~Data_Full.Name.isin(['Aumua Radewages', 'Madeleine Bordallo', 'Elizabeth Esty'])]\n",
    "\n",
    "# Fix Eric\n",
    "Data_Full.loc[Data_Full[Data_Full.Name == \"Erik Paulsen\"].index,\"Twitter\"] = \"ErikPaulsen\"\n",
    "\n",
    "# Fix Bobby\n",
    "Data_Full.loc[Data_Full[Data_Full.Name == \"Bobby Scott\"].index,\"Twitter\"] = \"BobbyScott\"\n",
    "\n",
    "# Fix Dave\n",
    "Data_Full.loc[Data_Full[Data_Full.Name == 'Dave Reichert'].index,\"Twitter\"] = \"TeamReichert\"\n",
    "\n",
    "# Fix Lindsey\n",
    "Data_Full.loc[Data_Full[Data_Full.Name == 'Lindsey Graham'].index,\"Twitter\"] = \"LindseyGrahamSC\"\n",
    "\n",
    "# Darin's name\n",
    "Data_Full.loc[Data_Full[Data_Full.Name == \"arin LaHood\"].index,\"Name\"] = \"Darin LaHood\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Full = Data_Full.drop_duplicates(subset = [\"Twitter\"], keep = 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Full = Data_Full.drop_duplicates(subset = [\"Name\"], keep = 'last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the President"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Full = Data_Full.append({'State': None, 'Party': 'R', 'Type': 'POTUS', 'Twitter': 'realDonaldTrump', 'Name': 'Donald J. Trump', 'twitter_display_name': 'Donald J. Trump'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_display_name = [api.get_user(handle).name for handle in Data_Full.Twitter]\n",
    "Data_Full['twitter_display_name'] = twitter_display_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Full.to_csv('../Data/Processed/Twitter_Handles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Clean-up of Harward Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = pd.read_pickle('../Data/Interim/congress.pkl')\n",
    "twitter_handles = pd.read_table('../Data/Processed/Twitter_Handles_updated.csv', sep = ',')\n",
    "\n",
    "s1 = set(twitter_handles['twitter_display_name'])\n",
    "s2 = set(congress.user_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#TonyCardenas🇺🇸',\n",
       " '(((Jerry Nadler)))',\n",
       " '@JasonSmithMO',\n",
       " 'Abigail Spanberger',\n",
       " 'Adam Smith',\n",
       " 'Alex Mooney',\n",
       " 'Alexandria Ocasio-Cortez',\n",
       " 'Andy Barr',\n",
       " 'Andy Harris',\n",
       " 'Andy Kim',\n",
       " 'Andy Levin',\n",
       " 'Ann Kirkpatrick',\n",
       " 'Annie Kuster',\n",
       " 'Anthony Brindisi',\n",
       " 'Anthony G. Brown',\n",
       " 'Anthony Gonzalez',\n",
       " 'Antonio Delgado',\n",
       " 'Archive: Senator McCaskill Office',\n",
       " 'Archive: Tom Graves',\n",
       " 'Ayanna Pressley',\n",
       " 'Barbara Lee',\n",
       " 'Barry Loudermilk',\n",
       " 'Ben Cardin',\n",
       " 'Ben Cline',\n",
       " 'Ben McAdams',\n",
       " 'Ben Sasse',\n",
       " 'Bill Flores',\n",
       " 'Bill Keating',\n",
       " 'Billy Long',\n",
       " 'Bishop for Congress',\n",
       " 'Blake Farenthold',\n",
       " 'Bob Casey Jr.',\n",
       " 'Bob Latta',\n",
       " 'Bobby Scott',\n",
       " 'Brad Schneider',\n",
       " 'Brad Sherman',\n",
       " 'Bradley Byrne',\n",
       " 'Brett Guthrie',\n",
       " 'Brian Fitzpatrick',\n",
       " 'Brian Schatz',\n",
       " 'Bruce Westerman',\n",
       " 'Capito for WV',\n",
       " 'Carlos Curbelo',\n",
       " 'Carolyn B. Maloney',\n",
       " 'Cathy McMorris Rodgers',\n",
       " 'Cedric Richmond',\n",
       " 'Charlie Dent',\n",
       " 'Cheri Bustos',\n",
       " 'Chip Roy',\n",
       " 'Chris Pappas',\n",
       " 'Chris Stewart',\n",
       " 'Chris Van Hollen',\n",
       " 'Cindy Axne',\n",
       " 'Cindy Hyde-Smith',\n",
       " 'Claire McCaskill',\n",
       " 'Claudia Tenney',\n",
       " 'Cleaver For Congress 🗳',\n",
       " 'Cohen for Congress',\n",
       " 'Colin Allred',\n",
       " 'Congressman Bob Brady',\n",
       " 'Congressman Dan Bishop',\n",
       " 'Congressman Fred Keller',\n",
       " 'Congressman Jeff Van Drew',\n",
       " 'Congressman Raja Krishnamoorthi',\n",
       " 'Congresswoman Tenney',\n",
       " 'Dan Crenshaw',\n",
       " 'Dan Kildee',\n",
       " 'Dan Lipinski',\n",
       " 'Dan Meuser',\n",
       " 'Dan Sullivan',\n",
       " 'Darren Soto',\n",
       " 'Dave Loebsack for Iowa',\n",
       " 'Dave Trott',\n",
       " 'David Cicilline',\n",
       " 'David Schweikert',\n",
       " 'David Trone',\n",
       " 'Dean Phillips 🇺🇸',\n",
       " 'Deb Fischer',\n",
       " 'Deb Haaland',\n",
       " 'Debbie Dingell',\n",
       " 'Debbie Lesko',\n",
       " 'Debbie Mucarsel-Powell',\n",
       " 'Debbie Stabenow',\n",
       " 'Debbie Wasserman Schultz',\n",
       " 'Dennis Ross',\n",
       " 'Denver Lee Riggleman III',\n",
       " 'Derek Kilmer',\n",
       " 'Diana DeGette',\n",
       " 'Dianne Feinstein',\n",
       " 'Dick Durbin',\n",
       " 'Don Bacon',\n",
       " 'Don Beyer',\n",
       " 'Don Young',\n",
       " 'Donald J. Trump',\n",
       " 'Donna E. Shalala',\n",
       " 'Doug Collins',\n",
       " 'Dr. Ami Bera',\n",
       " 'Dr. Kim Schrier',\n",
       " 'Dr. Ralph Abraham',\n",
       " 'Dr. Raul Ruiz',\n",
       " 'Dusty Johnson',\n",
       " 'Dutch for Congress',\n",
       " 'Dwight Evans',\n",
       " 'Ed Perlmutter',\n",
       " 'Elaine Luria',\n",
       " 'Elise Stefanik',\n",
       " 'Elissa Slotkin',\n",
       " 'Elizabeth Esty',\n",
       " 'Eric Swalwell',\n",
       " 'Eshoo for Congress',\n",
       " 'Frank LoBiondo',\n",
       " 'Frank Pallone, Jr.',\n",
       " 'Fred Upton',\n",
       " 'Garret Graves',\n",
       " 'Gary Peters',\n",
       " 'Gerry Connolly',\n",
       " 'Gil Cisneros',\n",
       " 'Glenn Grothman',\n",
       " 'Governor Kristi Noem',\n",
       " 'Grace Napolitano',\n",
       " 'Greg Pence',\n",
       " 'Greg Stanton',\n",
       " 'Greg Steube',\n",
       " 'Greg Walden',\n",
       " 'Guy Reschenthaler',\n",
       " 'Gwen S. Moore',\n",
       " 'Haley Stevens',\n",
       " 'Hank Johnson',\n",
       " 'Harley Rouda',\n",
       " 'Higgins for Congress',\n",
       " 'Jared Golden for Congress',\n",
       " 'Jason Chaffetz',\n",
       " 'Jason Crow',\n",
       " 'Jeanne Shaheen',\n",
       " 'Jeff Duncan',\n",
       " 'Jeff Merkley',\n",
       " 'Jennifer Wexton',\n",
       " 'Jesús “Chuy” García',\n",
       " 'Jim Bridenstine',\n",
       " 'Jim Clyburn SC-06',\n",
       " 'Jim Hagedorn',\n",
       " 'Jim McGovern',\n",
       " 'Jim Risch for US Senate',\n",
       " 'Jimmy Gomez',\n",
       " 'Jody Hice',\n",
       " 'Joe Courtney',\n",
       " 'Joe Crowley',\n",
       " 'Joe Cunningham',\n",
       " 'Joe Kennedy III',\n",
       " 'Joe Manchin',\n",
       " 'Joe Neguse',\n",
       " 'John Boozman',\n",
       " 'John Carter',\n",
       " 'John Conyers, Jr.',\n",
       " 'John Curtis',\n",
       " 'John Kennedy',\n",
       " 'John Larson',\n",
       " 'John McCain',\n",
       " 'John Rutherford',\n",
       " 'John Sarbanes',\n",
       " 'John Thune',\n",
       " 'Jon Tester',\n",
       " 'Josh Harder',\n",
       " 'Julia Brownley',\n",
       " 'Just Dave',\n",
       " 'Karen Bass',\n",
       " 'Kathy Castor',\n",
       " 'Katie Hill',\n",
       " 'Katie Porter',\n",
       " 'Keith Murdoch',\n",
       " 'Kelly Armstrong',\n",
       " 'Ken Buck',\n",
       " 'Kendra Horn',\n",
       " 'Kevin Brady for Congress',\n",
       " 'Kevin Cramer',\n",
       " 'King for Congress',\n",
       " 'Korey Young',\n",
       " 'Kweisi Mfume',\n",
       " 'Lamar Alexander',\n",
       " 'LangevinForCongress',\n",
       " 'Lauren Underwood',\n",
       " 'Leader McConnell',\n",
       " 'Linda Sánchez',\n",
       " 'Lisa Murkowski',\n",
       " 'Lizzie Pannill Fletcher',\n",
       " 'Lloyd Smucker',\n",
       " 'Lois Frankel',\n",
       " 'Lori Trahan',\n",
       " 'Lou Correa',\n",
       " 'Louise Slaughter',\n",
       " 'Lucy McBath',\n",
       " 'Luther Strange',\n",
       " 'MJ Hegar',\n",
       " 'Mac Thornberry',\n",
       " 'Madeleine Dean',\n",
       " 'Marc Veasey',\n",
       " 'Marcia L. Fudge',\n",
       " 'Maria Cantwell',\n",
       " 'Mark E. Green, MD',\n",
       " 'Mark James',\n",
       " 'Mark Pocan',\n",
       " 'Mark Warner HQ',\n",
       " 'Marsha Blackburn',\n",
       " 'Martha McSally for U.S. Senate',\n",
       " 'Mary Gay Scanlon',\n",
       " 'Matsui for Congress',\n",
       " 'Max Rose',\n",
       " 'Mazie Hirono',\n",
       " 'McHenry for Congress',\n",
       " 'Menendez for NJ',\n",
       " 'Michael Cloud',\n",
       " 'Michael F.Q. San Nicolas',\n",
       " 'Michael Guest',\n",
       " 'Michael Waltz',\n",
       " 'Mike (Wear a Mask) Thompson',\n",
       " 'Mike Bost',\n",
       " 'Mike Fitzpatrick',\n",
       " 'Mike Gallagher',\n",
       " 'Mike Garcia',\n",
       " 'Mike Johnson',\n",
       " 'Mike Levin',\n",
       " 'Mike Rogers Campaign',\n",
       " 'Mike Simpson',\n",
       " 'Mike Turner',\n",
       " 'Mikie Sherrill',\n",
       " 'Mitt Romney',\n",
       " 'Nanette D. Barragán',\n",
       " 'Nate McMurray for Congress 2020',\n",
       " 'Norma Torres',\n",
       " 'Nydia M Velázquez',\n",
       " 'Pat Roberts',\n",
       " 'Pat Tiberi',\n",
       " 'Pat Toomey',\n",
       " 'Paul D. Tonko',\n",
       " 'Paul Gosar',\n",
       " 'Paul Mitchell',\n",
       " 'Paul Ryan',\n",
       " 'Pete Aguilar',\n",
       " 'Pete King',\n",
       " 'Pete Visclosky',\n",
       " 'Ralph Norman',\n",
       " 'Rand Paul',\n",
       " 'Rashida Tlaib',\n",
       " 'Rep Andy Biggs',\n",
       " 'Rep Josh Gottheimer',\n",
       " 'Rep. Arrington',\n",
       " 'Rep. Brian Mast',\n",
       " 'Rep. Clay Higgins',\n",
       " 'Rep. Dan Bishop',\n",
       " 'Rep. David Kustoff',\n",
       " 'Rep. Devin Nunes',\n",
       " 'Rep. Donald McEachin',\n",
       " 'Rep. Ilhan Omar',\n",
       " 'Rep. Jamie Raskin',\n",
       " 'Rep. Jimmy Panetta',\n",
       " 'Rep. John Rutherford',\n",
       " 'Rep. Katie Hill',\n",
       " 'Rep. Lisa Blunt Rochester',\n",
       " 'Rep. Liz Cheney',\n",
       " 'Rep. Lloyd Doggett',\n",
       " 'Rep. Lloyd Smucker',\n",
       " 'Rep. Matt Gaetz',\n",
       " 'Rep. Mike Gallagher',\n",
       " 'Rep. Mike Kelly',\n",
       " 'Rep. Paul Mitchell',\n",
       " 'Rep. Pete Olson',\n",
       " 'Rep. Pramila Jayapal',\n",
       " 'Rep. Raúl Grijalva',\n",
       " 'Rep. Ro Khanna',\n",
       " 'Rep. Ruben J. Kihuen',\n",
       " 'Rep. Salud Carbajal',\n",
       " 'Rep. Stephanie Murphy',\n",
       " \"Rep. Tom O'Halleran\",\n",
       " 'Rep. Val Demings',\n",
       " 'Rick Larsen',\n",
       " 'Rob Wittman',\n",
       " 'Roger Wicker',\n",
       " 'Ron DeSantis',\n",
       " 'Ron Estes',\n",
       " 'Ron Johnson',\n",
       " 'Ron Kind',\n",
       " 'Ross Spano',\n",
       " 'RoyBluntMO',\n",
       " 'Ruben J. Kihuen',\n",
       " 'Russ Fulcher',\n",
       " 'Ryan Zinke',\n",
       " 'Scott Peters',\n",
       " 'Sean Casten',\n",
       " 'Sean Duffy',\n",
       " 'Sen. Cory Booker',\n",
       " 'Sen. Grassley Press',\n",
       " 'Sen. Jeff Van Drew',\n",
       " 'Sen. Kirsten Gillibrand',\n",
       " 'Sen. Maggie Hassan',\n",
       " 'Sen. Murphy Office',\n",
       " 'Senator Amy Klobuchar',\n",
       " 'Senator Doug Jones',\n",
       " 'Senator Hawley Press Office',\n",
       " 'Senator Mike Braun',\n",
       " 'Senator Rubio Press',\n",
       " 'Senator S',\n",
       " 'Senator Thad Cochran',\n",
       " 'Seth Moulton',\n",
       " 'Sharice Davids',\n",
       " 'Steny Hoyer',\n",
       " 'Stephen F. Lynch',\n",
       " 'Steve Chabot',\n",
       " 'Steve Watkins',\n",
       " 'Steve Womack',\n",
       " 'Steven Horsford',\n",
       " 'Steven Palazzo',\n",
       " 'Susan Brooks',\n",
       " 'Susan Wild',\n",
       " 'Susie Lee',\n",
       " 'Suzan DelBene',\n",
       " 'TJ Cox',\n",
       " 'Tammy Baldwin',\n",
       " 'Team GT - Friends of Glenn Thompson',\n",
       " 'Team Joe Wilson',\n",
       " 'Team McCaul',\n",
       " 'Team Meadows',\n",
       " 'Team Reichert',\n",
       " 'Ted Cruz',\n",
       " 'Ted Deutch',\n",
       " 'Ted Lieu',\n",
       " 'Thom Tillis',\n",
       " 'Tim Burchett',\n",
       " 'Tim Ryan',\n",
       " 'Tina Smith',\n",
       " 'Todd Young',\n",
       " 'Tom Carper',\n",
       " 'Tom Malinowski',\n",
       " 'Tom Price',\n",
       " 'Tom Tiffany',\n",
       " 'Tom Udall Press',\n",
       " 'Troy Balderson',\n",
       " 'Tulsi Gabbard 🌺',\n",
       " 'U.S. Senator Al Franken',\n",
       " 'U.S. Senator Bill Cassidy, M.D.',\n",
       " 'US Rep Brendan Boyle',\n",
       " 'US Rep. Al Lawson Jr',\n",
       " 'Van Drew for Congress',\n",
       " 'Van Taylor',\n",
       " 'Veronica Escobar',\n",
       " 'Vicky Hartzler',\n",
       " 'Walberg for Congress',\n",
       " 'Whitehouse for Senate',\n",
       " 'Will Hurd',\n",
       " 'William Timmons',\n",
       " 'Wyden for Oregon',\n",
       " 'WydenPress',\n",
       " 'Xochitl Torres Small',\n",
       " 'Yvette Clarke',\n",
       " 'Yvette D. Clarke for Congress',\n",
       " 'Zoe Lofgren'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_overlapping_twitter_profiles = s1 ^ s2\n",
    "non_overlapping_twitter_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure tweets only comes from people that twitter handles exist for. \n",
    "congress = congress[congress.user_name.isin(s1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the periods from Harward:\n",
    "mask = (\n",
    "    #January 27, 2017 and January 2, 2019 \n",
    "    (congress.created_at > '2017-1-27 00:00:00') & (congress.created_at < '2019-1-2 00:00:00')\n",
    "    | \n",
    "    #January 27, 2019 and May 7, 2020 \n",
    "    (congress.created_at > '2019-1-27 00:00:00') & (congress.created_at < '2020-5-7 00:00:00')\n",
    ")\n",
    "congress = congress[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = congress.drop_duplicates(keep='first')\n",
    "congress = congress.sort_values(by='created_at')\n",
    "congress = congress.reset_index(drop=True)\n",
    "congress.to_pickle(\"Data/interim/congress_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tweets ids and convert them to integers\n",
    "ids = list(congress.id.astype(int).values)\n",
    "\n",
    "filepath = \"../Data/raw/tweets/Cleaned_tweet_id.txt\"\n",
    "with open(filepath, 'w') as output:\n",
    "    for row in ids:\n",
    "        output.write(str(row) + '\\n')\n",
    "\n",
    "    print(f'{len(ids)} tweet ids saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1 Shortcut to exstract the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe with tweets of from congress after cleanup contain 60 % rows. \n",
    "\n",
    "Note running the cell below take $10 \\pm 2.5$ hours as the twitter API set limits to how much can be exstracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_tweet_id = np.loadtxt(\"../Data/Raw/Tweets/Cleaned_tweet_id.txt\", dtype=int)\n",
    "filepath = \"../Data/interim/congress_cleaned.pkl\"\n",
    "\n",
    "hydrate_tweets(\n",
    "    tweet_ids=congress_tweet_id,\n",
    "    filepath=filepath,\n",
    "    api = api\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Preprocess the twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = pd.read_pickle('../Data/Interim/congress_cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_characters = \",._´&’%':€$£!?#\"\n",
    "character_set = {\n",
    "    \"characters\": \"abcdefghijklmnopqrstuvwxyz0123456789\" + special_characters,\n",
    "    \"space\": \" \",\n",
    "}\n",
    "alphabet = \"\".join(character_set.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_links = re.compile(\"http\\S+\")\n",
    "regex_whitespace = re.compile(\"[\\s|-]+\")\n",
    "regex_unknown = re.compile(f\"[^{alphabet}]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_html_tags = {\n",
    "    \"&amp\": \"and\",\n",
    "    \"&lt\": \"<\",\n",
    "    \"&gt\": \">\",\n",
    "    \"&quot\": '\"',\n",
    "    \"&apos\": \"'\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace unicode charetars\n",
    "for pattern_string, char in regex_html_tags.items():\n",
    "    congress_tweets[\"text\"] = congress_tweets[\"text\"].str.replace(pattern_string, char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_tweets[\"text\"] = (congress_tweets[\"text\"]\n",
    "    .str.lower()\n",
    "    .str.replace(regex_links, \"\")\n",
    "    .str.replace(regex_whitespace, character_set[\"space\"])\n",
    "    .str.replace(regex_unknown, '')\n",
    "    .str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_tweets.to_pickle('../Data/Processed/congress_cleaned_processed.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('social_graphs_project': conda)",
   "language": "python",
   "name": "python37964bitsocialgraphsprojectconda9e1414738f16463880d93a451ffa336f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
